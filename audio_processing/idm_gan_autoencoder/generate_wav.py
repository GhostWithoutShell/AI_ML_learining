import torch
import numpy as np
import librosa
import soundfile as sf # pip install soundfile
import os
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–π —Å–≤–æ–∏ –∫–ª–∞—Å—Å—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (IDMAutoencoder, PositionalEncoding, etc.)
# –ß—Ç–æ–±—ã –Ω–µ –∫–æ–ø–∏–ø–∞—Å—Ç–∏—Ç—å, –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, –æ–Ω–∏ –≤ —Ñ–∞–π–ª–µ architecture.py
# –ò–ª–∏ —Å–∫–æ–ø–∏—Ä—É–π –∏—Ö —Å—é–¥–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é, –∫–∞–∫ –≤ –ø—Ä–æ—à–ª—ã–π —Ä–∞–∑.
from idm_gan_processing import IDMAutoencoder, IDMTensorDataset 

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
SR = 22050
N_FFT = 1024
HOP_LENGTH = 256
N_MELS = 64

def tensor_to_audio(tensor):
    """–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä [1, 64, Time] –æ–±—Ä–∞—Ç–Ω–æ –≤ –∑–≤—É–∫"""
    # 1. –¢–µ–Ω–∑–æ—Ä -> Numpy
    spec = tensor.squeeze().cpu().detach().numpy()
    
    # 2. –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–∏–∑ [0, 1] –æ–±—Ä–∞—Ç–Ω–æ –≤ [-80, 0] dB)
    spec_db = (spec * 80) - 80
    
    # 3. dB -> Power
    spec_power = librosa.db_to_power(spec_db)
    
    # 4. Mel -> Linear STFT (–ü—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ)
    # Griffin-Lim —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª–∏–Ω–µ–π–Ω—ã–º STFT, –∞ –Ω–µ Mel.
    # Librosa —É–º–µ–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ.
    stft_spec = librosa.feature.inverse.mel_to_stft(
        spec_power, sr=SR, n_fft=N_FFT, power=2.0
    )
    
    # 5. Griffin-Lim (–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∞–∑—ã)
    audio = librosa.griffinlim(stft_spec, n_iter=32, hop_length=HOP_LENGTH)
    
    return audio

def generate_morphing_track(model, dataset, device):
    print("üéß –ì–µ–Ω–µ—Ä–∏—Ä—É—é –Ω–æ–≤—ã–π IDM —Ç—Ä–µ–∫...")
    model.eval()
    
    # –ë–µ—Ä–µ–º –¥–≤–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ç—Ä–µ–∫–∞
    idx1 = np.random.randint(0, len(dataset))
    idx2 = np.random.randint(0, len(dataset))
    while idx1 == idx2: idx2 = np.random.randint(0, len(dataset))
    
    track_A = dataset[idx1].unsqueeze(0).to(device)
    track_B = dataset[idx2].unsqueeze(0).to(device)
    
    # === –ü–û–õ–£–ß–ê–ï–ú –õ–ê–¢–ï–ù–¢–´ ===
    with torch.no_grad():
        # Encoder A
        feat_A = model.encoder_cnn(track_A)
        b, c, h, w = feat_A.shape
        flat_A = model.pos_emb(feat_A.permute(0, 3, 1, 2).reshape(b, w, c*h))
        latent_A = model.transformer(flat_A)
        
        # Encoder B
        feat_B = model.encoder_cnn(track_B)
        flat_B = model.pos_emb(feat_B.permute(0, 3, 1, 2).reshape(b, w, c*h))
        latent_B = model.transformer(flat_B)
        
        # === –ú–û–†–§–ò–ù–ì (50%) ===
        # –°–º–µ—à–∏–≤–∞–µ–º –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
        latent_mix = (latent_A + latent_B) / 2
        
        # Decoder
        latent_reshaped = latent_mix.reshape(b, w, c, h).permute(0, 2, 3, 1)
        reconstructed_mix = model.decoder_cnn(latent_reshaped)

    # === –°–û–•–†–ê–ù–ï–ù–ò–ï ===
    # 1. –û—Ä–∏–≥–∏–Ω–∞–ª –ê
    audio_A = tensor_to_audio(track_A)
    sf.write('output_track_A.wav', audio_A, SR)
    
    # 2. –û—Ä–∏–≥–∏–Ω–∞–ª –ë
    audio_B = tensor_to_audio(track_B)
    sf.write('output_track_B.wav', audio_B, SR)
    
    # 3. –ù–ê–® –ì–ï–ù–ï–†–ê–¢–ò–í–ù–´–ô –¢–†–ï–ö
    audio_mix = tensor_to_audio(reconstructed_mix)
    sf.write('generated_IDM_hybrid.wav', audio_mix, SR)
    
    print("–ì–æ—Ç–æ–≤–æ! –°–ª—É—à–∞–π —Ñ–∞–π–ª 'generated_IDM_hybrid.wav' üéπ")

if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model = IDMAutoencoder().to(device)
    
    # –í–ê–ñ–ù–û: –¢—É—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ñ–∞–π–ª –≤–µ—Å–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã–π –Ω–∞ MEL-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞—Ö!
    weights_path = 'idm_generator_gan.pth' 
    
    if os.path.exists(weights_path):
        model.load_state_dict(torch.load(weights_path, map_location=device))
        print("–í–µ—Å–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
    else:
        print("–í–µ—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏ –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö idm_mels.")
        exit()

    # –î–∞—Ç–∞—Å–µ—Ç
    dataset = IDMTensorDataset(tensor_folder='idm_mels/train', slice_len=3, hop_length=256)
    
    generate_morphing_track(model, dataset, device)