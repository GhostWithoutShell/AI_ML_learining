import torch
import torch.nn as nn
from torch.utils.data import Dataset
import numpy as np
import os
import matplotlib.pyplot as plt
import glob
import math
import librosa
import soundfile as sf
from tqdm import tqdm

# ==========================================
# 1. –ê–†–•–ò–¢–ï–ö–¢–£–†–ê (–¢–≤–æ—è –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è)
# ==========================================
# (–í—Å—Ç–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Å—ã, —á—Ç–æ–±—ã —Å–∫—Ä–∏–ø—Ç –±—ã–ª –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–º)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(1), :].unsqueeze(0)

class IDMAutoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.embed_dim = 64 * 16
        self.pos_emb = PositionalEncoding(d_model=self.embed_dim, max_len=2000)
        
        self.encoder_cnn = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=(5, 3), stride=(2, 2), padding=(2, 1)),
            nn.BatchNorm2d(32), nn.GELU(),
            nn.Conv2d(32, 64, kernel_size=(5, 3), stride=(2, 2), padding=(2, 1)),
            nn.BatchNorm2d(64), nn.GELU()
        )
        encoder_layer = nn.TransformerEncoderLayer(d_model=self.embed_dim, nhead=8, dim_feedforward=2048, dropout=0.1, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=3)
        
        self.decoder_cnn = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.PixelShuffle(upscale_factor=2),
            nn.BatchNorm2d(32), nn.GELU(),
            nn.Conv2d(32, 16 * 4, kernel_size=3, padding=1), 
            nn.PixelShuffle(upscale_factor=2), nn.GELU(),
            nn.Conv2d(16, 1, kernel_size=3, padding=1),
            nn.Sigmoid()
        )

class IDMTensorDataset(Dataset):
    def __init__(self, tensor_folder, slice_len=3, sr=22050, hop_length=256):
        self.files = glob.glob(os.path.join(tensor_folder, '*.pt'))
        if len(self.files) == 0: self.files = glob.glob(os.path.join(tensor_folder, '**', '*.pt'), recursive=True)
        self.slice_pixels = int((slice_len * sr) / hop_length)
        self.slice_pixels = self.slice_pixels - (self.slice_pixels % 4)
    def __len__(self): return len(self.files)
    def __getitem__(self, idx):
        full_tensor = torch.load(self.files[idx])
        _, _, width = full_tensor.shape
        if width > self.slice_pixels:
            start = np.random.randint(0, width - self.slice_pixels)
            return full_tensor[:, :, start : start + self.slice_pixels]
        return full_tensor[:, :, :self.slice_pixels] # Simplification for demo

# ==========================================
# 2. –ì–ï–ù–ï–†–ê–¢–û–† –ò–°–ö–£–°–°–¢–í–ê (Gradient Visualizer)
# ==========================================

def tensor_to_audio(tensor, sr=22050, n_fft=1024, hop_length=256):
    spec = tensor.squeeze().cpu().detach().numpy()
    spec_db = (spec * 80) - 80
    spec_power = librosa.db_to_power(spec_db)
    stft_spec = librosa.feature.inverse.mel_to_stft(spec_power, sr=sr, n_fft=n_fft, power=2.0)
    audio = librosa.griffinlim(stft_spec, n_iter=32, hop_length=hop_length)
    return audio

def generate_av_art(model, dataset, device, frames=120):
    print("üé® –ù–∞—á–∏–Ω–∞—é —Å–æ–∑–¥–∞–Ω–∏–µ –∞—É–¥–∏–æ-–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞—Ä—Ç–∞...")
    os.makedirs("art_frames", exist_ok=True)
    
    # 1. –í—ã–±–∏—Ä–∞–µ–º —Ç—Ä–µ–∫–∏
    idx1, idx2 = np.random.randint(0, len(dataset)), np.random.randint(0, len(dataset))
    while idx1 == idx2: idx2 = np.random.randint(0, len(dataset))
    
    track_A = dataset[idx1].unsqueeze(0).to(device)
    track_B = dataset[idx2].unsqueeze(0).to(device)
    
    model.eval()
    
    # 2. –ü–æ–ª—É—á–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –∏ –∫–æ–Ω–µ—á–Ω—ã–π –ª–∞—Ç–µ–Ω—Ç
    # –ù–∞–º –Ω—É–∂–Ω–æ –≤—ã—Ç–∞—â–∏—Ç—å Z (–ª–∞—Ç–µ–Ω—Ç) –î–û —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, —á—Ç–æ–±—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–µ–∫–ª–∏ —á–µ—Ä–µ–∑ –Ω–µ–≥–æ
    def get_latent_pre_transformer(x):
        feat = model.encoder_cnn(x)
        b, c, h, w = feat.shape
        flat = feat.permute(0, 3, 1, 2).reshape(b, w, c*h)
        flat = model.pos_emb(flat)
        return flat, (b, c, h, w)

    with torch.no_grad():
        latent_A, shape = get_latent_pre_transformer(track_A)
        latent_B, _ = get_latent_pre_transformer(track_B)
    
    b, c, h, w = shape
    full_audio = []

    # 3. –¶–ò–ö–õ –ì–ï–ù–ï–†–ê–¶–ò–ò –ö–ê–î–†–û–í
    # –ú—ã –±—É–¥–µ–º –∏–¥—Ç–∏ –æ—Ç 0% –¥–æ 100% –º–æ—Ä—Ñ–∏–Ω–≥–∞
    alphas = np.linspace(0, 1, frames)
    
    print("üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–¥—Ä–æ–≤ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤...")
    
    for i, alpha in enumerate(tqdm(alphas)):
        # --- –ê. –°–ú–ï–®–ò–í–ê–ù–ò–ï ---
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –ª–∞—Ç–µ–Ω—Ç–∞, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–π –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        z_mix = (1 - alpha) * latent_A + alpha * latent_B
        
        # ! –ú–ê–ì–ò–Ø –ó–î–ï–°–¨ !
        # –ú—ã —Ä–∞–∑—Ä–µ—à–∞–µ–º PyTorch —Å—á–∏—Ç–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞
        z_mix = z_mix.detach().requires_grad_(True)
        
        # --- –ë. –ü–†–Ø–ú–û–ô –ü–†–û–•–û–î (FORWARD) ---
        # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏ –î–µ–∫–æ–¥–µ—Ä
        z_transformed = model.transformer(z_mix)
        latent_reshaped = z_transformed.reshape(b, w, c, h).permute(0, 2, 3, 1)
        generated_spec = model.decoder_cnn(latent_reshaped)
        
        # --- –í. –û–ë–†–ê–¢–ù–´–ô –ü–†–û–•–û–î (BACKWARD) ---
        # –ú—ã —Ö–æ—Ç–∏–º —É–∑–Ω–∞—Ç—å: –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ –ª–∞—Ç–µ–Ω—Ç–∞ —Å–∏–ª—å–Ω–µ–µ –≤—Å–µ–≥–æ –≤–ª–∏—è—é—Ç –Ω–∞ "–≥—Ä–æ–º–∫–æ—Å—Ç—å" –∫–∞—Ä—Ç–∏–Ω–∫–∏?
        # –ò–ª–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∏—Å–ø–µ—Ä—Å–∏—é (variance), —á—Ç–æ–±—ã –∏—Å–∫–∞—Ç—å –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ –º–µ—Å—Ç–∞
        target = generated_spec.sum() # –ü—Ä–æ—Å—Ç–∞—è —Å—É–º–º–∞ –≤—Å–µ—Ö –ø–∏–∫—Å–µ–ª–µ–π
        
        # –û–±–Ω—É–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
        model.zero_grad()
        if z_mix.grad is not None:
            z_mix.grad.zero_()
            
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–æ–ª–Ω—É –Ω–∞–∑–∞–¥
        target.backward()
        
        # --- –ì. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ì–†–ê–î–ò–ï–ù–¢–û–í ---
        # z_mix.grad - —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ [1, Time, 1024]
        grads = z_mix.grad.abs().squeeze().cpu().numpy()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã (—á—Ç–æ–±—ã –≤—Å–ø—ã—à–∫–∏ –±—ã–ª–∏ —è—Ä–∫–∏–º–∏)
        # –û—Ç—Ä–µ–∑–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã –¥–ª—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞
        grads = np.clip(grads, 0, np.percentile(grads, 99))
        grads = grads / (grads.max() + 1e-8)
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ [1024, Time]
        grads = grads.T 
        
        # --- –î. –û–¢–†–ò–°–û–í–ö–ê –ö–ê–î–†–ê ---
        # –°–≤–µ—Ä—Ö—É: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞ (–ó–≤—É–∫)
        # –°–Ω–∏–∑—É: –ö–∞—Ä—Ç–∞ –ì—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (–ú–æ–∑–≥ —Å–µ—Ç–∏)
        
        gen_img = generated_spec.detach().squeeze().cpu().numpy()
        
        plt.figure(figsize=(10, 10), facecolor='black')
        
        # –í–µ—Ä—Ö–Ω—è—è —á–∞—Å—Ç—å: –°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞
        plt.subplot(2, 1, 1)
        plt.imshow(gen_img, aspect='auto', origin='lower', cmap='magma')
        plt.axis('off')
        plt.title("Generated Sound (Decoder Output)", color='white', fontsize=10)
        
        # –ù–∏–∂–Ω—è—è —á–∞—Å—Ç—å: –ì–†–ê–î–ò–ï–ù–¢–´ (Neural Activity)
        plt.subplot(2, 1, 2)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º cmap='inferno' –∏–ª–∏ 'plasma' –¥–ª—è "–º–∞–≥–∏—á–µ—Å–∫–æ–≥–æ" –≤–∏–¥–∞
        plt.imshow(grads, aspect='auto', origin='lower', cmap='inferno') 
        plt.axis('off')
        plt.title("Neural Gradients (Sensitivity Map)", color='white', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(f"art_frames/frame_{i:04d}.png", facecolor='black')
        plt.close()
        
        # --- –ï. –°–ë–û–† –ê–£–î–ò–û ---
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–≤—É–∫ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∫–∞–¥—Ä–∞
        # –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–æ –º–µ–¥–ª–µ–Ω–Ω–æ. –î–ª—è –¥–µ–º–æ –±–µ—Ä–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é —á–∞—Å—Ç—å
        current_audio = tensor_to_audio(generated_spec.detach())
        
        # –ß—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –ø–ª–∞–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –∑–≤—É–∫–∞, –º—ã –±–µ—Ä–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –∫—É—Å–æ—á–µ–∫ –∏–∑ —Ü–µ–Ω—Ç—Ä–∞
        # (–≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞, –¥–ª—è –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –º–æ—Ä—Ñ–∏–Ω–≥–∞ –∑–≤—É–∫–∞ –Ω—É–∂–Ω—ã –∫—Ä–æ—Å—Å—Ñ–µ–π–¥—ã)
        samples_per_frame = len(current_audio) // frames
        start_sample = 0 
        # –ü—Ä–æ—Å—Ç–æ –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –≤–µ—Å—å –∫—É—Å–æ–∫ (–±—É–¥–µ—Ç –Ω–∞–ª–æ–∂–µ–Ω–∏–µ, –Ω–æ –¥–ª—è IDM —Å–æ–π–¥–µ—Ç –∫–∞–∫ —Ç–µ–∫—Å—Ç—É—Ä–∞)
        # –î–ª—è —á–∏—Å—Ç–æ—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –ª—É—á—à–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ—Å—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–∞–¥—Ä —Ü–µ–ª–∏–∫–æ–º
        # –ù–æ –¥–∞–≤–∞–π —Å–æ—Ö—Ä–∞–Ω–∏–º –º–æ—Ä—Ñ–∏–Ω–≥:
        if i == 0:
            full_audio = current_audio
        else:
            # –ü—Ä–æ—Å—Ç–æ–π –∫—Ä–æ—Å—Å—Ñ–µ–π–¥ –Ω–µ –¥–µ–ª–∞–µ–º, –ø—Ä–æ—Å—Ç–æ —Å–∫–ª–µ–∏–≤–∞–µ–º (–¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞)
            # –õ—É—á—à–µ: –ø—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω–∏–º –∞—É–¥–∏–æ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ (50%) –∫–∞–∫ "–∑–≤—É–∫ —ç—Ç–æ–≥–æ –≤–∏–¥–µ–æ"
            # –ò–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–¥–∏–Ω –¥–ª–∏–Ω–Ω—ã–π —Ç—Ä–µ–∫ –æ—Ç–¥–µ–ª—å–Ω–æ.
            pass

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–ª–∏–Ω–Ω—ã–π –∞—É–¥–∏–æ —Ç—Ä–µ–∫ –º–æ—Ä—Ñ–∏–Ω–≥–∞ (–ø—Ä–∞–≤–∏–ª—å–Ω–æ)
    print("üéπ –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –∞—É–¥–∏–æ...")
    final_audio_list = []
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞—É–¥–∏–æ –∫—É—Å–∫–∞–º–∏ –ø–æ 10 –∫–∞–¥—Ä–æ–≤ —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∏–≤–∞—Ç—å –ø–∞–º—è—Ç—å, –Ω–æ –∑–¥–µ—Å—å –ø—Ä–æ—Å—Ç–æ –≤–æ–∑—å–º–µ–º
    # –∞—É–¥–∏–æ –∏–∑ —Å–µ—Ä–µ–¥–∏–Ω—ã (50%) –∫–∞–∫ –ø—Ä–∏–º–µ—Ä –∑–≤—É—á–∞–Ω–∏—è, 
    # –ª–∏–±–æ (–ª—É—á—à–µ) —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º 4 –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–∫–∏ –∏ —Å–∫–ª–µ–∏–º.
    
    # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã: —Å–æ—Ö—Ä–∞–Ω–∏–º –∞—É–¥–∏–æ –ü–û–°–õ–ï–î–ù–ï–ì–û –∫–∞–¥—Ä–∞ (track B) –∏ –°–†–ï–î–ù–ï–ì–û (Morph)
    # –ß—Ç–æ–±—ã —Ç—ã –º–æ–≥ –Ω–∞–ª–æ–∂–∏—Ç—å –∏—Ö –≤ –≤–∏–¥–µ–æ—Ä–µ–¥–∞–∫—Ç–æ—Ä–µ.
    
    mid_idx = frames // 2
    
    # –†–µ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ä–µ–¥–Ω–∏–π –∫–∞–¥—Ä –¥–ª—è –∑–≤—É–∫–∞
    z_mid = (0.5 * latent_A + 0.5 * latent_B)
    z_mid = model.transformer(z_mid)
    latent_reshaped = z_mid.reshape(b, w, c, h).permute(0, 2, 3, 1)
    spec_mid = model.decoder_cnn(latent_reshaped)
    audio_mid = tensor_to_audio(spec_mid.detach())
    
    sf.write('art_audio_mid.wav', audio_mid, 22050)
    print("–ì–æ—Ç–æ–≤–æ! –ö–∞–¥—Ä—ã –≤ –ø–∞–ø–∫–µ 'art_frames', –∑–≤—É–∫ 'art_audio_mid.wav'")

# ==========================================
# –ó–ê–ü–£–°–ö
# ==========================================
if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = IDMAutoencoder().to(device)
    
    # –£–ö–ê–ñ–ò –ü–£–¢–¨ –ö –í–ï–°–ê–ú (MEL-SPECTROGRAM VERSION)
    weights_path = 'idm_generator_gan.pth' 
    
    if os.path.exists(weights_path):
        model.load_state_dict(torch.load(weights_path, map_location=device))
    else:
        print("–í–µ—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        exit()

    dataset = IDMTensorDataset(tensor_folder='idm_mels/train', slice_len=3, hop_length=256)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 100 –∫–∞–¥—Ä–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω–æ 3-4 —Å–µ–∫—É–Ω–¥—ã –≤–∏–¥–µ–æ –ø—Ä–∏ 30fps)
    generate_av_art(model, dataset, device, frames=100)